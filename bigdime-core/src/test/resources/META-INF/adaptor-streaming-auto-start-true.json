{
    "name": "unit-test-batch-adaptor",
    "type": "streaming",
    "cron-expression" : "0/5 * * * * ?",
    "auto-start" : "true",
    "namespace": "com.example",
    "description": "adaptor to ingest the data from tracking api into hdfs.",
    "source": {
        "name": "tracking-api-source",
        "description": "source description",
        "source-type": "file or oracle or mysql or kafka etc",
        "src-desc": {
            "input1" : "tab3",
            "input2" : "topic1:par3,par1"
        },
        "src-desc1": {
            "input1" : "tab3,tab4",
            "input2" : "topic1:par3,par1",
            "input3" : "topic2:par1"
        },
        "data-handlers": [
            {
                "name": "kafka-data-reader",
                "description": "read data from partitions specified with src-desc field",
                "handler-class": "io.bigdime.core.handler.DummyConcreteHandler",
                "properties": {
                    "broker-hosts": "kafka.provider.one:9092,kafka.provider.two:9096",
                    "offset-data-dir": "/tmp",
                    "message-size" : "20000"
                }
            },
            {
                "name": "avro-record-handler",
                "description": "Parse avro record and creates a JsonNode",
                "handler-class": "io.bigdime.core.handler.DummyConcreteHandler",
                "properties": {}
            },
            {
                "name": "input-processor",
                "description": "processes the data",
                "handler-class": "io.bigdime.core.handler.DummyConcreteHandler",
                "properties": {
                    "channel-map" : "input1:channel1, input2:channel2",
                    "arrayKey" : [
                    	"array1", "array2"
                    ],
                    "arrayNodeKey" : [
                    	{"name" : "nameValueArrayNode1", "id" : "idValueArrayNode1"},
                    	{"name" : "nameValueArrayNode2", "id" : "idValueArrayNode2"}
                    ]
                }
            }
        ]
    },
    "channel": [
        {
            "name": "channel1",
            "description" : "channel for us tracking data",
            "channel-class": "io.bigdime.core.channel.MemoryChannel",
            "properties": {
            	"initialCapacity" : "16",
            	"maxCapacity" : "1000000",
	            "concurrency": "3"
            }
        },
        {
            "name": "channel2",
            "description" : "channel for us tracking data",
            "channel-class": "io.bigdime.core.channel.MemoryChannel",
            "properties": {
            	"initialCapacity" : "16",
            	"maxCapacity" : "1000000",
	            "concurrency": "3"
            }
        }
    ],
    "sink": [
        {
            "name": "hdfs sink for tracking data adaptor",
            "description": "hdfs sink for tracking data adaptor",
            "channel-desc": ["channel1", "channel2"],
            "data-handlers": [
                {
                    "name": "memory-channel-reader",
                    "description": "read data from channels",
                    "handler-class": "io.bigdime.core.handler.MemoryChannelInputHandler",
                    "properties": {
                        
                    }
                }
            ]
        },
        {
            "name": "hbase sink for tracking data adaptor",
            "description": "hbase sink for tracking data adaptor",
            "channel-desc": ["channel1"],
            "data-handlers": [
                {
                    "name": "memory-channel-reader",
                    "description": "read data from channels",
                    "handler-class": "io.bigdime.core.handler.MemoryChannelInputHandler",
                    "properties": {
                        
                    }
                }
            ]
        }
   ]
}